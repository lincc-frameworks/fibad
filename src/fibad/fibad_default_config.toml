[general]
# Set to `true` during development to skip checking for default config values
# in external libraries. Use `false` otherwise.
dev_mode = false

# Destination of log messages. Options: 'stderr', 'stdout' specify the console,
# "path/to/fibad.log" specifies a file.
log_destination = "stderr"

# Lowest log level to emit. Options: "critical", "error", "warning", "info", "debug".
log_level = "info"

# Directory where data is stored.
data_dir = "./data"

#Top level directory for writing results.
results_dir = "./results"


[download]
# Cut out width in arcseconds.
sw = "22asec"

# Cut out height in arcseconds.
sh = "22asec"

# The filters to download.
filter = ["HSC-G", "HSC-R", "HSC-I", "HSC-Z", "HSC-Y"]

# The type of data to download.
type = "coadd"

# The data release to download from.
rerun = "pdr3_wide"

# Path to credentials.ini file for the downloader. File contents should be:
# username = "<your username>"
# password = "<your password>"
credentials_file = "./credentials.ini"

# Alternative method to pass credentials to the downloader. Prefer to use a
#credentials.ini file to avoid exposing credentials with source control.
username = false
password = false

# The number of sources to download from the catalog. Default is -1, which
# downloads all sources in the catalog.
num_sources = -1

# The number of sources to skip in the catalog. Default is 0.
offset = 0

# The number of concurrent connections to use when downloading data.
concurrent_connections = 4

# The number of seconds between printing download statistics.
stats_print_interval = 60

# The path to the catalog file that defines which cutouts to download.
fits_file = "./catalog.fits"

# The number of seconds to wait before retrying a failed HTTP request in seconds.
retry_wait = 30

# How many times to retry a failed HTTP request before moving on to the next one.
retries = 3

# Number of seconds to wait for a full HTTP response from the server.
timeout = 3600

# The number of sky location rectangles should we request in a single request.
chunk_size = 990

# Request the image layer from the cutout service
image = true

# Request the variance layer from the cutout service
variance = false

# Request the mask layer from the cutout service
mask = false


[model]
# The name of the model to use. Option are a builtin model class name or import path
# to an external model. e.g. "ExampleAutoencoder", "user_pkg.model.ExternalModel"
name = "ExampleAutoencoder"

# The number of output channels from the first layer.
base_channel_size = 32

# The length of the latent space vector.
latent_dim = 64


[criterion]
# The name of the built-in criterion to use or the libpath to an external criterion
name = "torch.nn.CrossEntropyLoss"

[optimizer]
# The name of the built-in optimizer to use or the libpath to an external optimizer
name = "torch.optim.SGD"

# Default PyTorch optimizer parameters. The keys match the names of the parameters
lr = 0.01
momentum = 0.9

[train]
weights_filepath = "example_model.pth"
epochs = 10
# Set this to the path of a checkpoint file to resume, or continue training,
# from a checkpoint. Otherwise, set to false to start from scratch.
resume = false
split = "train"

[data_set]
# Name of the built-in data loader to use or the libpath to an external data loader
# e.g. "user_package.submodule.ExternalDataLoader" or "HSCDataSet"
name = "CifarDataSet"

# Pixel dimensions used to crop all images prior to loading. Will prune any images that are too small.
#
# If not provided by user, the default of 'false' scans the directory for the smallest dimensioned files, and 
# uses those pixel dimensions as the crop size.
#
#crop_to = [100,100]
crop_to = false

# Limit data loader to only particular filters when there are more in the data set.
#
# When not provided by the user, the number of filters will be automatically gleaned from the data set.
# Defaults behavior is produced by the false value.
#
#filters = ["HSC-G", "HSC-R", "HSC-I", "HSC-Z", "HSC-Y"]
filters = false

# A fits file which specifies object IDs to filter a large dataset in [general].data_dir down
# Implementation is dataset class dependent. Default is false meaning now filtering.
filter_catalog = false

# How to split the data between training and eval sets.
# The semantics are borrowed from scikit-learn's train-test-split, and HF Dataset's train-test-split function
# It is an error for these values to add to more than 1.0 as ratios or the size of the dataset if expressed
# as integers.

# train_size: Size of the train split
# If `float`, should be between `0.0` and `1.0` and represent the proportion of the dataset to include in the 
# train split.
# If `int`, represents the absolute number of train samples.
# If `false`, the value is automatically set to the complement of the test size.
train_size = 0.6

# validate_size: Size of the validation split
# If `float`, should be between `0.0` and `1.0` and represent the proportion of the dataset to include in the 
# train split.
# If `int`, represents the absolute number of train samples.
# If `false`, and both train_size and test_size are defined, the value is automatically set to the complement 
# of the other two sizes summed.
# If `false`, and only one of the other sizes is defined, no validate split is created
validate_size = 0.2

# test_size: Size of the test split
# If `float`, should be between `0.0` and `1.0` and represent the proportion of the dataset to include in the 
# test split.
# If `int`, represents the absolute number of test samples.
# If `false`, the value is set to the complement of the train size.
# If `train_size` is also `false`, it will be set to `0.25`.
test_size = 0.2

# Number to seed with for generating a random split. False means the data will be seeded from
# a system source at runtime.
seed = false

#Controls whether images are cached during data loading. For training, this reduces runtimes
#after the first epoch.
use_cache = true

[data_loader]
# Default PyTorch DataLoader parameters
batch_size =32

# We could remove this potentially - pytorch-ignite will default to shuffle=True
# If the user wanted to explicitly require no shuffling, they could set this to false.
shuffle = false
num_workers = 2

[infer]
model_weights_file = false
# Select a split ("train", "test", or "validate") to use for inference
# default of false selects the entire dataset for inference.
split = false
