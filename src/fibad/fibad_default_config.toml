[general]
use_gpu = true

# Destination of log messages
# 'stderr' and 'stdout' specify the console.
log_destination = "stderr"
# A path name specifies a file e.g.
# log = "fibad_log.txt"

# Lowest log level to emit.
# As you go down the list, fibad will become more verbose in the log.
#
# log_level = "critical" # Only emit the most severe of errors
# log_level = "error"    # Emit all errors
# log_level = "warning"  # Emit warnings and all errors
log_level = "info"     # Emit informational messages, warnings and all errors
# log_level = "debug"    # Very verbose, emit all log messages.

data_dir = "./data"
results_dir = "./results" # Results get named <verb>-<timestamp> under this directory

[download]
sw = "22asec"
sh = "22asec"
filter = ["HSC-G", "HSC-R", "HSC-I", "HSC-Z", "HSC-Y"]
type = "coadd"
rerun = "pdr3_wide"
username = false
password = false
num_sources = -1 # Values below 1 here indicate all sources in the catalog will be downloaded
offset = 0
concurrent_connections = 4
stats_print_interval = 60
fits_file = "./catalog.fits"

# These control the downloader's HTTP requests and retries
# `retry_wait` How long to wait before retrying a failed HTTP request in seconds. Default 30s
retry_wait = 30
# `retries` How many times to retry a failed HTTP request before moving on to the next one. Default 3 times
retries = 3
# `timepout` How long should we wait to get a full HTTP response from the server. Default 3600s (1hr)
timeout = 3600
# `chunksize` How many sky location rectangles should we request in a single request. Default is 990
chunk_size = 990

# Whether to retrieve the image layer
image = true
# Whether to retrieve the variance layer
variance = false
# Whether to retrieve the mask layer
mask = false

[model]
# The name of the built-in model to use or the libpath to an external model
# e.g. "user_package.submodule.ExternalModel" or "ExampleAutoencoder"
name = "ExampleAutoencoder"

base_channel_size = 32
latent_dim = 64

[train]
weights_filepath = "example_model.pth"
epochs = 10
# Set this to the path of a checkpoint file to resume, or continue training,
# from a checkpoint. Otherwise, set to false to start from scratch.
resume = false
split = "train"

[data_set]
# Name of the built-in data loader to use or the libpath to an external data loader
# e.g. "user_package.submodule.ExternalDataLoader" or "HSCDataLoader"
name = "CifarDataSet"

# Pixel dimensions used to crop all images prior to loading. Will prune any images that are too small.
#
# If not provided by user, the default of 'false' scans the directory for the smallest dimensioned files, and 
# uses those pixel dimensions as the crop size.
#
#crop_to = [100,100]
crop_to = false

# Limit data loader to only particular filters when there are more in the data set.
#
# When not provided by the user, the number of filters will be automatically gleaned from the data set.
# Defaults behavior is produced by the false value.
#
#filters = ["HSC-G", "HSC-R", "HSC-I", "HSC-Z", "HSC-Y"]
filters = false

[data_loader]
# Default PyTorch DataLoader parameters
batch_size = 32
shuffle = true
num_workers = 2

[prepare]
# How to split the data between training and eval sets.
# The semantics are borrowed from scikit-learn's train-test-split, and HF Dataset's train-test-split function
# It is an error for these values to add to more than 1.0 as ratios or the size of the dataset if expressed
# as integers.

# train_size: Size of the train split
# If `float`, should be between `0.0` and `1.0` and represent the proportion of the dataset to include in the 
# train split.
# If `int`, represents the absolute number of train samples.
# If `false`, the value is automatically set to the complement of the test size.
train_size = 0.6

# validate_size: Size of the validation split
# If `float`, should be between `0.0` and `1.0` and represent the proportion of the dataset to include in the 
# train split.
# If `int`, represents the absolute number of train samples.
# If `false`, and both train_size and test_size are defined, the value is automatically set to the complement 
# of the other two sizes summed.
# If `false`, and only one of the other sizes is defined, no validate split is created
validate_size = 0.2

# test_size: Size of the test split
# If `float`, should be between `0.0` and `1.0` and represent the proportion of the dataset to include in the 
# test split.
# If `int`, represents the absolute number of test samples.
# If `false`, the value is set to the complement of the train size.
# If `train_size` is also `false`, it will be set to `0.25`.
test_size = 0.6

# Number to seed with for generating a random split. False means the data will be seeded from
# a system source at runtime.
seed = false

[predict]
model_weights_file = false
batch_size = 32
split = "test"
